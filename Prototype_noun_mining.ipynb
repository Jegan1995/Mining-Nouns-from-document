{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":19,"outputs":[{"output_type":"stream","text":"/kaggle/input/func_spec_doc.docx\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PRE INSTALLED LIBRARIES IN ANACONDA PROMPT BEFORE RUNNING CODE\n#pip install docx2txt\n#pip install fpdf\n\n# IMPORT LIBRARIES\nimport os\nimport sys\nimport traceback\nimport numpy as np\nimport pandas as ps\nimport nltk\nimport re\nimport docx2txt                     # To convert .doc to .txt file\nimport csv\nimport string\n#from nltk.corpus import brown\nfrom fpdf import FPDF               # To create pdf file and write data\nfrom nltk.corpus import stopwords   # To remove stop words \nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n#TextAnalytic Class to mine nouns from document\nclass TextAnalytic:  \n    \n    # init method or constructor    \n    def doc_init(self,count):\n        self.count = 0  \n        \n        \n    # Method to open document   \n    def ReadDocument(self,doc_path):\n        \n        try:\n            source = docx2txt.process(doc_path)\n            return source \n        \n        # If any error occurs Exception is called    \n        except Exception as e:\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            formatted_lines = traceback.format_exc().splitlines()\n            \"\"\"\n            exception handled here and above two line is generic exception\n            \"\"\"\n            print('Error while reading document',formatted_lines)\n    \n    \n    # Method to read document and do noun mining in dcument\n    def MineNoun(self,read_doc):\n        try:\n            # Using nltk.pos_tag to segregate nouns\n            min_nouns = [nouns for (nouns, pos) in nltk.pos_tag(nltk.word_tokenize(read_doc.lower())) if pos[0] == 'N']\n            \n            # To remove any stop words found after tagging nouns\n            stop_words = set(stopwords.words('english')) \n            filtered_nouns = [w for w in min_nouns  if not w in stop_words]\n            \n            # To remove any unwanted punctuation after filtering stop words\n            mined_nouns = list(filter(lambda filtered_nouns: filtered_nouns not in string.punctuation and len(filtered_nouns)>1, filtered_nouns))\n            \n            #To indicate successful processing nouns from document\n            print('Successfully mined documents')\n            \n            #To create .pdf file with pages\n            pdf = FPDF() \n            pdf.add_page() \n            count=0\n            \n            #To create and write data into .csv file\n            fields = ['Mined Nouns']\n            rows = [[mined_nouns]]\n            with open(\"Output1.csv\", 'w') as csvfile:    \n                csvwriter = csv.writer(csvfile)  \n                csvwriter.writerow(fields)  \n                csvwriter.writerows(rows) \n    \n            #To create and write data into .txt file\n            file1= open(\"Output1.txt\",\"w+\")\n            mined = str.join('     ',mined_nouns)\n            file1.write(\"Mined Nouns\")\n            file1.write(mined)\n            file1.close()\n\n            #To write data into .pdf file in iteration\n            for i in mined_nouns:\n                pdf.set_font(\"Arial\", size = 15) \n                pdf.cell(200, 10, txt = i, ln = count, align = 'L') \n                count = count + 1\n            pdf.output(\"Output1.pdf\")\n            \n        # If any error occurs Exception is called    \n        except Exception as e:\n            exc_type, exc_value, exc_traceback = sys.exc_info()\n            formatted_lines = traceback.format_exc().splitlines()\n            \"\"\"\n            exception handled here and above two line is generic exception\n            \"\"\"\n            print('Error while creating mined pdf ',formatted_lines)\n\n            \n#Instantiating the class object            \ndoc_obj = TextAnalytic()     \n\n#Passing input parameters in the doc_path below.\ndoc_path = r\"../input/func_spec_doc.docx\"\n\n\n# To read the document, path is provided as input\nread_doc = doc_obj.ReadDocument(doc_path)\n\n# To indicate Successful read of document\nprint(\"Successfully read input document\")\n\n\n#To process the document data it is passed on \ncreate_output = doc_obj.MineNoun(read_doc)\n\n# To Print the successful execution of output \nprint(\"Successfully created Output file\")","execution_count":20,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\nSuccessfully read input document\nSuccessfully mined documents\nSuccessfully created Output file\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# INPUT DOCUMENT SAMPLE\nsource = docx2txt.process(r\"../input/func_spec_doc.docx\")\nprint(source)","execution_count":21,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/specification5/func_spec_doc.docx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-daf4f171ab8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# INPUT DOCUMENT SAMPLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"../input/specification5/func_spec_doc.docx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/docx2txt/docx2txt.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(docx, img_dir)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# unzip the docx in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mfilelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/specification5/func_spec_doc.docx'"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}